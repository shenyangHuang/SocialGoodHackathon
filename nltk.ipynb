{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'pwd' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-11-60b85248170b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mnltk\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mpwd\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'pwd' is not defined"
     ]
    }
   ],
   "source": [
    "import nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from nltk.tokenize import *\n",
    "str1 = ''.join([i for i in nltk.data.load('andy.txt') if not i.isdigit()])\n",
    "str2 = ''.join([i for i in nltk.data.load('andy2.txt') if not i.isdigit()])\n",
    "str3 = ''.join([i for i in nltk.data.load('andy3.txt') if not i.isdigit()])\n",
    "str4 = ''.join([i for i in nltk.data.load('movie.txt') if not i.isdigit()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "tokenizer = TweetTokenizer()\n",
    "documents = [str1,str2,str3,str4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "from nltk.corpus import stopwords\n",
    "from collections import Counter\n",
    "import re\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "stopwords = stopwords.words('english')\n",
    "addon = ['done','dude']\n",
    "stopwords.extend(addon)\n",
    "TermFrequency = []\n",
    "#To improve use textRank method \n",
    "\n",
    "\n",
    "for i in range(0,len(documents)):\n",
    "    tokens = tokenizer.tokenize(documents[i])\n",
    "    results = []\n",
    "    #lemmatize the tokens\n",
    "    for token in tokens:\n",
    "        result = lemmatizer.lemmatize(token)\n",
    "        results.append(result)\n",
    "    words=[]\n",
    "    #removing common words\n",
    "    for result in results:\n",
    "        word = result.lower()\n",
    "        words.append(word)\n",
    "    words = [x for x in words if x not in stopwords]\n",
    "    #remove signs like , ; ?\n",
    "    Wds = []\n",
    "    signs = [',','!','=','?','!','@','~','$','\"','+','.','-','...']\n",
    "    for word in words:\n",
    "        inside = False\n",
    "        for sign in signs: \n",
    "            if sign == word :\n",
    "                inside = True\n",
    "        if inside == False:\n",
    "            Wds.append(word)\n",
    "    TermFrequency.append( Counter(Wds) )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Counter({'sure': 1, 'think': 1, 'work': 4}),\n",
       " Counter({'cream': 2,\n",
       "          'ice': 2,\n",
       "          'lose': 1,\n",
       "          'man': 1,\n",
       "          'time': 1,\n",
       "          'want': 3,\n",
       "          'weight': 1}),\n",
       " Counter({'everything': 1,\n",
       "          'frequency': 1,\n",
       "          'get': 1,\n",
       "          'hackathon': 1,\n",
       "          'happy': 1,\n",
       "          'hopefully': 1,\n",
       "          'interesting': 1,\n",
       "          \"let's\": 1,\n",
       "          'love': 1,\n",
       "          'really': 1,\n",
       "          'see': 1,\n",
       "          'starting': 1,\n",
       "          'term': 1,\n",
       "          'work': 1}),\n",
       " Counter({\"'\": 3,\n",
       "          'always': 2,\n",
       "          \"aren't\": 1,\n",
       "          'art': 1,\n",
       "          'attachment': 1,\n",
       "          'babble': 1,\n",
       "          'back': 1,\n",
       "          'become': 1,\n",
       "          'believe': 1,\n",
       "          'bianca': 31,\n",
       "          'blonde': 1,\n",
       "          'blowdryer': 1,\n",
       "          'boring': 1,\n",
       "          'boyfriend': 1,\n",
       "          'breed': 1,\n",
       "          \"c'esc\": 1,\n",
       "          'cameron': 35,\n",
       "          \"can't\": 2,\n",
       "          'cause': 1,\n",
       "          'champagne': 1,\n",
       "          'chastity': 1,\n",
       "          'chat': 1,\n",
       "          'coiffure': 1,\n",
       "          'conditioner': 1,\n",
       "          'cost': 1,\n",
       "          'could': 2,\n",
       "          'counted': 1,\n",
       "          'crap': 2,\n",
       "          'date': 4,\n",
       "          'dating': 1,\n",
       "          'day': 1,\n",
       "          'deep': 1,\n",
       "          \"didn't\": 1,\n",
       "          'diffuser': 1,\n",
       "          'dip': 1,\n",
       "          'doe': 2,\n",
       "          \"doesn't\": 1,\n",
       "          \"don't\": 2,\n",
       "          'drawer': 1,\n",
       "          'easy': 1,\n",
       "          \"eber's\": 1,\n",
       "          'endless': 1,\n",
       "          'enough': 1,\n",
       "          'eventually': 1,\n",
       "          'ever': 3,\n",
       "          'every': 1,\n",
       "          'extra': 1,\n",
       "          'failing': 1,\n",
       "          'fault': 1,\n",
       "          'fear': 1,\n",
       "          'figured': 1,\n",
       "          'find': 2,\n",
       "          'forget': 2,\n",
       "          'found': 1,\n",
       "          'french': 1,\n",
       "          'fun': 1,\n",
       "          'gentleman': 1,\n",
       "          'get': 3,\n",
       "          'go': 5,\n",
       "          'god': 1,\n",
       "          'goin': 1,\n",
       "          'going': 1,\n",
       "          'gonna': 2,\n",
       "          'good': 3,\n",
       "          'gosh': 1,\n",
       "          'got': 2,\n",
       "          'guillermo': 1,\n",
       "          'guy': 2,\n",
       "          'hair': 1,\n",
       "          'harboring': 1,\n",
       "          'head': 2,\n",
       "          'hear': 1,\n",
       "          'heard': 1,\n",
       "          'help': 1,\n",
       "          'hi': 1,\n",
       "          'hideous': 1,\n",
       "          'high': 1,\n",
       "          'hope': 1,\n",
       "          'huh': 1,\n",
       "          \"i'm\": 5,\n",
       "          \"i've\": 1,\n",
       "          'instructor': 1,\n",
       "          'introduction': 1,\n",
       "          \"it's\": 2,\n",
       "          'jared': 1,\n",
       "          'kat': 1,\n",
       "          'kidding': 1,\n",
       "          'kind': 1,\n",
       "          'know': 6,\n",
       "          'l': 64,\n",
       "          'learn': 1,\n",
       "          'lesbian': 1,\n",
       "          'let': 1,\n",
       "          \"let's\": 1,\n",
       "          'leto': 1,\n",
       "          'lie': 1,\n",
       "          'life': 1,\n",
       "          'lighter': 1,\n",
       "          'like': 10,\n",
       "          'listen': 1,\n",
       "          'little': 1,\n",
       "          'look': 2,\n",
       "          'looked': 1,\n",
       "          'looks': 1,\n",
       "          'loser': 1,\n",
       "          'mercy': 1,\n",
       "          'might': 1,\n",
       "          'mind': 1,\n",
       "          'much': 1,\n",
       "          'mystery': 1,\n",
       "          'need': 1,\n",
       "          'never': 3,\n",
       "          'nice': 1,\n",
       "          'obviously': 1,\n",
       "          'occupied': 1,\n",
       "          'okay': 2,\n",
       "          'one': 4,\n",
       "          'particularly': 1,\n",
       "          'party': 1,\n",
       "          'pastel': 1,\n",
       "          'persona': 1,\n",
       "          'picture': 1,\n",
       "          'plan': 1,\n",
       "          'point': 1,\n",
       "          'popular': 1,\n",
       "          'pretty': 2,\n",
       "          'progressing': 1,\n",
       "          'proper': 1,\n",
       "          'quit': 1,\n",
       "          'quiz': 1,\n",
       "          'ready': 1,\n",
       "          'real': 1,\n",
       "          'really': 4,\n",
       "          'right': 1,\n",
       "          'same-sex': 1,\n",
       "          'say': 4,\n",
       "          'school': 1,\n",
       "          'see': 2,\n",
       "          'seem': 1,\n",
       "          'seemed': 1,\n",
       "          'seems': 1,\n",
       "          'selfish': 1,\n",
       "          'shame': 1,\n",
       "          'share': 1,\n",
       "          \"she'd\": 1,\n",
       "          \"she's\": 2,\n",
       "          'sick': 1,\n",
       "          'sister': 2,\n",
       "          'smoke': 1,\n",
       "          'someone': 2,\n",
       "          'something': 2,\n",
       "          'sometimes': 1,\n",
       "          'started': 1,\n",
       "          'store': 1,\n",
       "          'story': 1,\n",
       "          'stuff': 3,\n",
       "          'sure': 2,\n",
       "          'sweet': 1,\n",
       "          'tendency': 1,\n",
       "          'tete': 1,\n",
       "          'thank': 1,\n",
       "          \"that's\": 4,\n",
       "          \"there's\": 1,\n",
       "          'thing': 3,\n",
       "          'think': 1,\n",
       "          'though': 1,\n",
       "          'thug': 1,\n",
       "          'tonight': 2,\n",
       "          'tons': 1,\n",
       "          'two': 1,\n",
       "          'u': 64,\n",
       "          'unless': 1,\n",
       "          'unsolved': 1,\n",
       "          'use': 1,\n",
       "          'used': 1,\n",
       "          'useful': 1,\n",
       "          'wa': 2,\n",
       "          'wanna': 1,\n",
       "          'want': 2,\n",
       "          'wanted': 1,\n",
       "          'wearing': 1,\n",
       "          'well': 2,\n",
       "          'wench': 1,\n",
       "          'without': 1,\n",
       "          'word': 1,\n",
       "          'worked': 1,\n",
       "          'workin': 1,\n",
       "          'wow': 1,\n",
       "          \"you'd\": 1,\n",
       "          \"you're\": 4})]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "TermFrequency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#nltk sentiment analysis\n",
    "from nltk.tokenize import sent_tokenize\n",
    "from nltk.classify import NaiveBayesClassifier\n",
    "from nltk.corpus import subjectivity\n",
    "from nltk.sentiment import SentimentAnalyzer \n",
    "from nltk.sentiment.util import *\n",
    "#divide text into sentences\n",
    "sentences = []\n",
    "for document in documents:\n",
    "    lines = document.lower().split('\\n')\n",
    "    for line in lines:\n",
    "        sentences.append(sent_tokenize(line))\n",
    "Sents = [x for x in sentences if x]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "this will work i am sure\n",
      "compound: 0.3182, neg: 0.0, neu: 0.635, pos: 0.365, \n",
      "this will work i think so\n",
      "compound: 0.0, neg: 0.0, neu: 1.0, pos: 0.0, \n",
      "this will work\n",
      "compound: 0.0, neg: 0.0, neu: 1.0, pos: 0.0, \n",
      "this will work\n",
      "compound: 0.0, neg: 0.0, neu: 1.0, pos: 0.0, \n",
      "i just wants more ice cream\n",
      "compound: 0.0, neg: 0.0, neu: 1.0, pos: 0.0, \n",
      "i want ice cream, dude\n",
      "compound: 0.0772, neg: 0.0, neu: 0.698, pos: 0.302, \n",
      "and i want to lose weight at the same time\n",
      "compound: -0.34, neg: 0.245, neu: 0.636, pos: 0.118, \n",
      "what do i do man\n",
      "compound: 0.0, neg: 0.0, neu: 1.0, pos: 0.0, \n",
      "what do i do!\n",
      "compound: 0.0, neg: 0.0, neu: 1.0, pos: 0.0, \n",
      "i am at a hackathon\n",
      "compound: 0.0, neg: 0.0, neu: 1.0, pos: 0.0, \n",
      "i love it\n",
      "compound: 0.6369, neg: 0.0, neu: 0.192, pos: 0.808, \n",
      "it is starting to get really interesting\n",
      "compound: 0.4576, neg: 0.0, neu: 0.667, pos: 0.333, \n",
      "and hopefully it works with term frequency and everything\n",
      "compound: 0.4019, neg: 0.0, neu: 0.748, pos: 0.252, \n",
      "let's see how it is done\n",
      "compound: 0.0, neg: 0.0, neu: 1.0, pos: 0.0, \n",
      "i am happy\n",
      "compound: 0.5719, neg: 0.0, neu: 0.213, pos: 0.787, \n",
      "l +++$+++ u +++$+++ m +++$+++ bianca +++$+++ they do not!\n",
      "compound: 0.0, neg: 0.0, neu: 1.0, pos: 0.0, \n",
      "l +++$+++ u +++$+++ m +++$+++ cameron +++$+++ they do to!\n",
      "compound: 0.0, neg: 0.0, neu: 1.0, pos: 0.0, \n",
      "l +++$+++ u +++$+++ m +++$+++ bianca +++$+++ i hope so.\n",
      "compound: 0.4404, neg: 0.0, neu: 0.674, pos: 0.326, \n",
      "l +++$+++ u +++$+++ m +++$+++ cameron +++$+++ she okay?\n",
      "compound: 0.2263, neg: 0.0, neu: 0.759, pos: 0.241, \n",
      "l +++$+++ u +++$+++ m +++$+++ bianca +++$+++ let's go.\n",
      "compound: 0.0, neg: 0.0, neu: 1.0, pos: 0.0, \n",
      "l +++$+++ u +++$+++ m +++$+++ cameron +++$+++ wow\n",
      "compound: 0.5859, neg: 0.0, neu: 0.568, pos: 0.432, \n",
      "l +++$+++ u +++$+++ m +++$+++ bianca +++$+++ okay -- you're gonna need to learn how to lie.\n",
      "compound: 0.2263, neg: 0.0, neu: 0.881, pos: 0.119, \n",
      "l +++$+++ u +++$+++ m +++$+++ cameron +++$+++ no\n",
      "compound: -0.296, neg: 0.306, neu: 0.694, pos: 0.0, \n",
      "l +++$+++ u +++$+++ m +++$+++ bianca +++$+++ i'm kidding.\n",
      "compound: 0.1027, neg: 0.0, neu: 0.811, pos: 0.189, \n",
      "l +++$+++ u +++$+++ m +++$+++ bianca +++$+++ like my fear of wearing pastels?\n",
      "compound: -0.1779, neg: 0.218, neu: 0.612, pos: 0.17, \n",
      "l +++$+++ u +++$+++ m +++$+++ cameron +++$+++ the \"real you\".\n",
      "compound: 0.0, neg: 0.0, neu: 1.0, pos: 0.0, \n",
      "l +++$+++ u +++$+++ m +++$+++ bianca +++$+++ what good stuff?\n",
      "compound: 0.4404, neg: 0.0, neu: 0.707, pos: 0.293, \n",
      "l +++$+++ u +++$+++ m +++$+++ cameron +++$+++ i figured you'd get to the good stuff eventually.\n",
      "compound: 0.4404, neg: 0.0, neu: 0.805, pos: 0.195, \n",
      "l +++$+++ u +++$+++ m +++$+++ cameron +++$+++ thank god!\n",
      "compound: 0.5983, neg: 0.0, neu: 0.505, pos: 0.495, \n",
      "l +++$+++ u +++$+++ m +++$+++ bianca +++$+++ me.\n",
      "compound: 0.0, neg: 0.0, neu: 1.0, pos: 0.0, \n",
      "l +++$+++ u +++$+++ m +++$+++ cameron +++$+++ what crap?\n",
      "compound: -0.3818, neg: 0.302, neu: 0.698, pos: 0.0, \n",
      "l +++$+++ u +++$+++ m +++$+++ bianca +++$+++ do you listen to this crap?\n",
      "compound: -0.4588, neg: 0.231, neu: 0.769, pos: 0.0, \n",
      "l +++$+++ u +++$+++ m +++$+++ cameron +++$+++ no...\n",
      "compound: 0.0, neg: 0.0, neu: 1.0, pos: 0.0, \n",
      "l +++$+++ u +++$+++ m +++$+++ bianca +++$+++ then guillermo says, \"if you go any lighter, you're gonna look like an extra on .\"\n",
      "compound: 0.3612, neg: 0.0, neu: 0.889, pos: 0.111, \n",
      "l +++$+++ u +++$+++ m +++$+++ cameron +++$+++ you always been this selfish?\n",
      "compound: -0.561, neg: 0.287, neu: 0.713, pos: 0.0, \n",
      "l +++$+++ u +++$+++ m +++$+++ bianca +++$+++ but\n",
      "compound: 0.0, neg: 0.0, neu: 1.0, pos: 0.0, \n",
      "l +++$+++ u +++$+++ m +++$+++ cameron +++$+++ then that's all you had to say.\n",
      "compound: 0.0, neg: 0.0, neu: 1.0, pos: 0.0, \n",
      "l +++$+++ u +++$+++ m +++$+++ bianca +++$+++ well, no...\n",
      "compound: 0.2732, neg: 0.0, neu: 0.741, pos: 0.259, \n",
      "l +++$+++ u +++$+++ m +++$+++ cameron +++$+++ you never wanted to go out with 'me, did you?\n",
      "compound: 0.0, neg: 0.0, neu: 1.0, pos: 0.0, \n",
      "l +++$+++ u +++$+++ m +++$+++ bianca +++$+++ i was?\n",
      "compound: 0.0, neg: 0.0, neu: 1.0, pos: 0.0, \n",
      "l +++$+++ u +++$+++ m +++$+++ cameron +++$+++ i looked for you back at the party, but you always seemed to be \"occupied\".\n",
      "compound: 0.2144, neg: 0.0, neu: 0.907, pos: 0.093, \n",
      "l +++$+++ u +++$+++ m +++$+++ bianca +++$+++ tons\n",
      "compound: 0.0, neg: 0.0, neu: 1.0, pos: 0.0, \n",
      "l +++$+++ u +++$+++ m +++$+++ cameron +++$+++ have fun tonight?\n",
      "compound: 0.5106, neg: 0.0, neu: 0.68, pos: 0.32, \n",
      "l +++$+++ u +++$+++ m +++$+++ cameron +++$+++ i believe we share an art instructor\n",
      "compound: 0.296, neg: 0.0, neu: 0.82, pos: 0.18, \n",
      "l +++$+++ u +++$+++ m +++$+++ bianca +++$+++ you know chastity?\n",
      "compound: 0.0, neg: 0.0, neu: 1.0, pos: 0.0, \n",
      "l +++$+++ u +++$+++ m +++$+++ cameron +++$+++ looks like things worked out tonight, huh?\n",
      "compound: 0.3612, neg: 0.0, neu: 0.815, pos: 0.185, \n",
      "l +++$+++ u +++$+++ m +++$+++ bianca +++$+++ hi.\n",
      "compound: 0.0, neg: 0.0, neu: 1.0, pos: 0.0, \n",
      "l +++$+++ u +++$+++ m +++$+++ bianca +++$+++ who knows?\n",
      "compound: 0.0, neg: 0.0, neu: 1.0, pos: 0.0, \n",
      "l +++$+++ u +++$+++ m +++$+++ cameron +++$+++ so that's the kind of guy she likes?\n",
      "compound: 0.4215, neg: 0.0, neu: 0.811, pos: 0.189, \n",
      "l +++$+++ u +++$+++ m +++$+++ bianca +++$+++ lesbian?\n",
      "compound: 0.0, neg: 0.0, neu: 1.0, pos: 0.0, \n",
      "l +++$+++ u +++$+++ m +++$+++ cameron +++$+++ she's not a...\n",
      "compound: 0.0, neg: 0.0, neu: 1.0, pos: 0.0, \n",
      "l +++$+++ u +++$+++ m +++$+++ cameron +++$+++ i'm workin' on it.\n",
      "compound: 0.0, neg: 0.0, neu: 1.0, pos: 0.0, \n",
      "l +++$+++ u +++$+++ m +++$+++ bianca +++$+++ i really, really, really wanna go, but i can't.\n",
      "compound: 0.0, neg: 0.0, neu: 1.0, pos: 0.0, \n",
      "l +++$+++ u +++$+++ m +++$+++ cameron +++$+++ sure have.\n",
      "compound: 0.3182, neg: 0.0, neu: 0.723, pos: 0.277, \n",
      "l +++$+++ u +++$+++ m +++$+++ bianca +++$+++ eber's deep conditioner every two days.\n",
      "compound: 0.0, neg: 0.0, neu: 1.0, pos: 0.0, \n",
      "l +++$+++ u +++$+++ m +++$+++ cameron +++$+++ how do you get your hair to look like that?\n",
      "compound: 0.3612, neg: 0.0, neu: 0.848, pos: 0.152, \n",
      "l +++$+++ u +++$+++ m +++$+++ bianca +++$+++ you're sweet.\n",
      "compound: 0.4588, neg: 0.0, neu: 0.667, pos: 0.333, \n",
      "l +++$+++ u +++$+++ m +++$+++ cameron +++$+++ you have my word.\n",
      "compound: 0.0, neg: 0.0, neu: 1.0, pos: 0.0, \n",
      "l +++$+++ u +++$+++ m +++$+++ bianca +++$+++ i counted on you to help my cause.\n",
      "compound: 0.4019, neg: 0.0, neu: 0.803, pos: 0.197, \n",
      "l +++$+++ u +++$+++ m +++$+++ cameron +++$+++ you got something on your mind?\n",
      "compound: 0.0, neg: 0.0, neu: 1.0, pos: 0.0, \n",
      "l +++$+++ u +++$+++ m +++$+++ bianca +++$+++ where?\n",
      "compound: 0.0, neg: 0.0, neu: 1.0, pos: 0.0, \n",
      "l +++$+++ u +++$+++ m +++$+++ cameron +++$+++ there.\n",
      "compound: 0.0, neg: 0.0, neu: 1.0, pos: 0.0, \n",
      "l +++$+++ u +++$+++ m +++$+++ cameron +++$+++ well, there's someone i think might be --\n",
      "compound: 0.2732, neg: 0.0, neu: 0.84, pos: 0.16, \n",
      "l +++$+++ u +++$+++ m +++$+++ bianca +++$+++ how is our little find the wench a date plan progressing?\n",
      "compound: 0.0, neg: 0.0, neu: 1.0, pos: 0.0, \n",
      "l +++$+++ u +++$+++ m +++$+++ bianca +++$+++ forget french.\n",
      "compound: -0.2263, neg: 0.241, neu: 0.759, pos: 0.0, \n",
      "l +++$+++ u +++$+++ m +++$+++ cameron +++$+++ that's because it's such a nice one.\n",
      "compound: 0.4215, neg: 0.0, neu: 0.781, pos: 0.219, \n",
      "l +++$+++ u +++$+++ m +++$+++ bianca +++$+++ i don't want to know how to say that though.\n",
      "compound: -0.0572, neg: 0.086, neu: 0.914, pos: 0.0, \n",
      "l +++$+++ u +++$+++ m +++$+++ cameron +++$+++ right.\n",
      "compound: 0.0, neg: 0.0, neu: 1.0, pos: 0.0, \n",
      "l +++$+++ u +++$+++ m +++$+++ bianca +++$+++ c'esc ma tete.\n",
      "compound: 0.0, neg: 0.0, neu: 1.0, pos: 0.0, \n",
      "l +++$+++ u +++$+++ m +++$+++ cameron +++$+++ let me see what i can do.\n",
      "compound: 0.0, neg: 0.0, neu: 1.0, pos: 0.0, \n",
      "l +++$+++ u +++$+++ m +++$+++ bianca +++$+++ gosh, if only we could find kat a boyfriend...\n",
      "compound: 0.0, neg: 0.0, neu: 1.0, pos: 0.0, \n",
      "l +++$+++ u +++$+++ m +++$+++ cameron +++$+++ that's a shame.\n",
      "compound: -0.4767, neg: 0.341, neu: 0.659, pos: 0.0, \n",
      "l +++$+++ u +++$+++ m +++$+++ bianca +++$+++ unsolved mystery.\n",
      "compound: 0.0, neg: 0.0, neu: 1.0, pos: 0.0, \n",
      "l +++$+++ u +++$+++ m +++$+++ cameron +++$+++ why?\n",
      "compound: 0.0, neg: 0.0, neu: 1.0, pos: 0.0, \n",
      "l +++$+++ u +++$+++ m +++$+++ cameron +++$+++ seems like she could get a date easy enough...\n",
      "compound: 0.6597, neg: 0.0, neu: 0.671, pos: 0.329, \n",
      "l +++$+++ u +++$+++ m +++$+++ bianca +++$+++ the thing is, cameron -- i'm at the mercy of a particularly hideous breed of loser.\n",
      "compound: -0.2263, neg: 0.142, neu: 0.753, pos: 0.105, \n",
      "l +++$+++ u +++$+++ m +++$+++ cameron +++$+++ cameron.\n",
      "compound: 0.0, neg: 0.0, neu: 1.0, pos: 0.0, \n",
      "l +++$+++ u +++$+++ m +++$+++ bianca +++$+++ no, no, it's my fault -- we didn't have a proper introduction ---\n",
      "compound: -0.7269, neg: 0.336, neu: 0.664, pos: 0.0, \n",
      "l +++$+++ u +++$+++ m +++$+++ cameron +++$+++ forget it.\n",
      "compound: -0.2263, neg: 0.241, neu: 0.759, pos: 0.0, \n"
     ]
    }
   ],
   "source": [
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "sid = SentimentIntensityAnalyzer()\n",
    "for sentence in Sents:\n",
    "    ss = sid.polarity_scores(sentence[0])\n",
    "    print (sentence[0])\n",
    "    for k in sorted(ss):\n",
    "        print('{0}: {1}, '.format(k, ss[k]), end='')\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "nltk.download()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "n_instances = 100\n",
    "subj_docs = [(sent, 'subj') for sent in subjectivity.sents(categories='subj')[:n_instances]]\n",
    "obj_docs = [(sent, 'obj') for sent in subjectivity.sents(categories='obj')[:n_instances]]\n",
    "len(subj_docs), len(obj_docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "train_subj_docs = subj_docs[:80]\n",
    "test_subj_docs = subj_docs[80:100]\n",
    "train_obj_docs = obj_docs[:80]\n",
    "test_obj_docs = obj_docs[80:100]\n",
    "training_docs = train_subj_docs+train_obj_docs\n",
    "testing_docs = test_subj_docs+test_obj_docs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "sentim_analyzer = SentimentAnalyzer()\n",
    "all_words_neg = sentim_analyzer.all_words([mark_negation(doc) for doc in training_docs])\n",
    "unigram_feats = sentim_analyzer.unigram_word_feats(all_words_neg, min_freq=4)\n",
    "sentim_analyzer.add_feat_extractor(extract_unigram_feats, unigrams=unigram_feats)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "training_set = sentim_analyzer.apply_features(training_docs)\n",
    "test_set = sentim_analyzer.apply_features(testing_docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "trainer = NaiveBayesClassifier.train\n",
    "classifier = sentim_analyzer.train(trainer, training_set)\n",
    "for key,value in sorted(sentim_analyzer.evaluate(test_set).items()):\n",
    "    print('{0}: {1}'.format(key, value))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
